100%|██████████| 38/38 [13:48<00:00, 21.79s/it][INFO|trainer.py:2584] 2025-04-14 02:03:15,831 >>
{'loss': 0.1856, 'grad_norm': 0.5297728984741478, 'learning_rate': 4.991461232516675e-06, 'epoch': 0.05}
{'loss': 0.1821, 'grad_norm': 0.7612377332750213, 'learning_rate': 4.789433316637644e-06, 'epoch': 0.25}
{'loss': 0.161, 'grad_norm': 0.5389666774075293, 'learning_rate': 4.1932039290643534e-06, 'epoch': 0.5}
{'loss': 0.1428, 'grad_norm': 0.5380565927179342, 'learning_rate': 3.3117486730117092e-06, 'epoch': 0.76}
{'loss': 0.1401, 'grad_norm': 0.7988879201395154, 'learning_rate': 2.2935516363191695e-06, 'epoch': 1.03}
{'loss': 0.0916, 'grad_norm': 0.47237283045937445, 'learning_rate': 1.3101315174073162e-06, 'epoch': 1.28}
{'loss': 0.083, 'grad_norm': 0.44474117042019135, 'learning_rate': 5.271487265090163e-07, 'epoch': 1.53}
{'loss': 0.0764, 'grad_norm': 0.41384626841811545, 'learning_rate': 7.649933515167407e-08, 'epoch': 1.78}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 38/38 [13:48<00:00, 21.81s/it]
{'train_runtime': 829.83, 'train_samples_per_second': 6.112, 'train_steps_per_second': 0.046, 'train_loss': 0.12123346603230427, 'epoch': 1.93}
***** train metrics *****
  epoch                    =     1.9338
  total_flos               =     9391GF
  train_loss               =     0.1212
  train_runtime            = 0:13:49.83
  train_samples            =       2536
  train_samples_per_second =      6.112
  train_steps_per_second   =      0.046
2025-04-14 02:03:15 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-14 02:03:20,039 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2
[INFO|configuration_utils.py:414] 2025-04-14 02:03:20,045 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:865] 2025-04-14 02:03:20,048 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 02:04:33,126 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 02:04:33,130 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 02:04:33,132 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-14 02:04:37,580 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2
[INFO|configuration_utils.py:414] 2025-04-14 02:04:37,587 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:865] 2025-04-14 02:04:37,589 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 02:05:45,923 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 02:05:45,927 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 02:05:45,929 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/special_tokens_map.json
events.out.tfevents.1744609766.h1compute00.ihc.umd.edu.1396920.0: 100%|██████████| 7.98k/7.98k [00:00<00:00, 177kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 43.3MB/s]1<01:38, 49.1MB/s]
training_args.bin: 100%|██████████| 7.16k/7.16k [00:00<00:00, 196kB/s]01:21, 59.0MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:21<00:00, 51.0MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:17<00:00, 55.9MB/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:33<00:00, 52.9MB/s]0/7.98k [00:00<?, ?B/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [01:35<00:00, 51.2MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:35<00:00, 13.61s/it]1:34<00:00, 60.6MB/s]
[INFO|configuration_utils.py:414] 2025-04-14 02:07:59,668 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/config.json
[INFO|trainer.py:3801] 2025-04-14 02:08:03,505 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2
[INFO|configuration_utils.py:414] 2025-04-14 02:08:03,511 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/config.json
2025-04-14 02:07:59 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2
2025-04-14 02:07:59 - INFO - __main__ - Pushing to hub...
[INFO|configuration_utils.py:865] 2025-04-14 02:08:03,514 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 02:09:14,753 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 02:09:14,756 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 02:09:14,757 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_2/special_tokens_map.json
2025-04-14 02:09:52 - INFO - __main__ - *** Training complete ***
