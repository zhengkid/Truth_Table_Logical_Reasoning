  0%|          | 0/12 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-04-06 19:35:35,129 >> It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
 17%|█▋        | 2/12 [01:47<08:54, 53.41s/it]
{'loss': 0.2936, 'grad_norm': 1.0044650021958308, 'learning_rate': 4.914814565722671e-06, 'epoch': 0.14}
