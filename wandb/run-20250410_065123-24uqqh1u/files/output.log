  0%|          | 0/58 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-04-10 06:51:24,595 >> It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
  2%|‚ñè         | 1/58 [00:57<54:15, 57.11s/it]
{'loss': 1.1486, 'grad_norm': 23.346340590156093, 'learning_rate': 4.99633353462781e-06, 'epoch': 0.03}
