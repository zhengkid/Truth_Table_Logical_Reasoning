100%|██████████| 38/38 [13:38<00:00, 21.49s/it][INFO|trainer.py:2584] 2025-04-14 17:23:59,349 >>
{'loss': 0.0733, 'grad_norm': 0.4635411478509184, 'learning_rate': 4.991461232516675e-06, 'epoch': 0.05}
{'loss': 0.0711, 'grad_norm': 0.8951173609930304, 'learning_rate': 4.789433316637644e-06, 'epoch': 0.25}
{'loss': 0.0635, 'grad_norm': 0.5917046685314882, 'learning_rate': 4.1932039290643534e-06, 'epoch': 0.51}
{'loss': 0.0615, 'grad_norm': 0.4862248097985252, 'learning_rate': 3.3117486730117092e-06, 'epoch': 0.76}
{'loss': 0.0595, 'grad_norm': 0.6356616592953854, 'learning_rate': 2.2935516363191695e-06, 'epoch': 1.02}
{'loss': 0.0397, 'grad_norm': 0.3433602550881641, 'learning_rate': 1.3101315174073162e-06, 'epoch': 1.27}
{'loss': 0.0373, 'grad_norm': 0.3792873225135197, 'learning_rate': 5.271487265090163e-07, 'epoch': 1.53}
{'loss': 0.0365, 'grad_norm': 0.3888793211982078, 'learning_rate': 7.649933515167407e-08, 'epoch': 1.78}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 38/38 [13:38<00:00, 21.53s/it]
{'train_runtime': 819.1295, 'train_samples_per_second': 6.133, 'train_steps_per_second': 0.046, 'train_loss': 0.05142474174499512, 'epoch': 1.94}
***** train metrics *****
  epoch                    =     1.9363
  total_flos               =     9354GF
  train_loss               =     0.0514
  train_runtime            = 0:13:39.12
  train_samples            =       2512
  train_samples_per_second =      6.133
  train_steps_per_second   =      0.046
2025-04-14 17:23:59 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-14 17:24:03,544 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 17:24:03,551 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 17:24:03,554 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 17:25:11,547 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 17:25:11,550 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 17:25:11,552 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-14 17:25:15,907 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 17:25:15,914 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 17:25:15,916 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 17:26:24,302 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 17:26:24,306 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 17:26:24,308 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/special_tokens_map.json
events.out.tfevents.1744665020.h1compute00.ihc.umd.edu.1464172.0: 100%|██████████| 7.98k/7.98k [00:00<00:00, 115kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 24.4MB/s]0<01:21, 59.6MB/s]
training_args.bin: 100%|██████████| 7.16k/7.16k [00:00<00:00, 218kB/s]01:21, 59.5MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:25<00:00, 42.2MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:47<00:00, 40.3MB/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:48<00:00, 45.3MB/s]0/7.98k [00:00<?, ?B/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [02:11<00:00, 37.1MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [02:11<00:00, 18.84s/it] :25<00:00, 56.3MB/s]
[INFO|configuration_utils.py:414] 2025-04-14 17:29:17,580 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/config.json
[INFO|trainer.py:3801] 2025-04-14 17:29:21,519 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 17:29:21,525 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/config.json
2025-04-14 17:29:17 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3
2025-04-14 17:29:17 - INFO - __main__ - Pushing to hub...
[INFO|configuration_utils.py:865] 2025-04-14 17:29:21,528 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 17:30:34,954 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 17:30:34,957 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 17:30:34,959 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_3/special_tokens_map.json
2025-04-14 17:31:13 - INFO - __main__ - *** Training complete ***
