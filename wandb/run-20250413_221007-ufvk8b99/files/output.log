100%|██████████| 36/36 [13:07<00:00, 21.91s/it][INFO|trainer.py:2584] 2025-04-13 22:23:16,361 >>
{'loss': 1.3507, 'grad_norm': 32.81523178809005, 'learning_rate': 4.990486745229364e-06, 'epoch': 0.06}
{'loss': 0.783, 'grad_norm': 13.31229353115199, 'learning_rate': 4.765769467591626e-06, 'epoch': 0.28}
{'loss': 0.369, 'grad_norm': 1.9147447525650767, 'learning_rate': 4.106969024216348e-06, 'epoch': 0.55}
{'loss': 0.3207, 'grad_norm': 0.8153124821993536, 'learning_rate': 3.147047612756302e-06, 'epoch': 0.83}
{'loss': 0.3524, 'grad_norm': 0.7354791167914929, 'learning_rate': 2.0658795558326745e-06, 'epoch': 1.11}
{'loss': 0.28, 'grad_norm': 0.7170515593806996, 'learning_rate': 1.0660589091223854e-06, 'epoch': 1.38}
{'loss': 0.2678, 'grad_norm': 0.6893568263330226, 'learning_rate': 3.3493649053890325e-07, 'epoch': 1.66}
{'loss': 0.2587, 'grad_norm': 0.6410639284613692, 'learning_rate': 9.513254770636138e-09, 'epoch': 1.94}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 36/36 [13:07<00:00, 21.88s/it]
{'train_runtime': 788.9365, 'train_samples_per_second': 5.846, 'train_steps_per_second': 0.046, 'train_loss': 0.3879436171717114, 'epoch': 1.99}
***** train metrics *****
  epoch                    =     1.9931
  total_flos               =     8126GF
  train_loss               =     0.3879
  train_runtime            = 0:13:08.93
  train_samples            =       2306
  train_samples_per_second =      5.846
  train_steps_per_second   =      0.046
2025-04-13 22:23:16 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-13 22:23:20,928 >> Saving model checkpoint to /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-13 22:23:20,937 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-13 22:23:20,939 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-13 22:24:32,156 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-13 22:24:32,162 >> tokenizer config file saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-13 22:24:32,164 >> Special tokens file saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-13 22:24:36,962 >> Saving model checkpoint to /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-13 22:24:36,969 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-13 22:24:36,971 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-13 22:25:50,761 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-13 22:25:50,766 >> tokenizer config file saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-13 22:25:50,768 >> Special tokens file saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/special_tokens_map.json
events.out.tfevents.1744596607.h1compute00.ihc.umd.edu.1373160.0: 100%|██████████| 8.29k/8.29k [00:00<00:00, 66.2kB/s]
tokenizer.json: 100%|██████████| 17.2M/17.2M [00:00<00:00, 41.8MB/s]0<01:07, 73.2MB/s]
training_args.bin: 100%|██████████| 7.42k/7.42k [00:00<00:00, 143kB/s]01:34, 52.0MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.17G/1.17G [00:27<00:00, 42.0MB/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.98G/4.98G [01:32<00:00, 53.6MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.92G/4.92G [01:34<00:00, 51.9MB/s]0/8.29k [00:00<?, ?B/s]
model-00002-of-00004.safetensors: 100%|██████████| 5.00G/5.00G [01:37<00:00, 51.2MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:37<00:00, 13.98s/it]1:35<00:00, 57.4MB/s]
[INFO|configuration_utils.py:414] 2025-04-13 22:28:21,641 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/config.json
[INFO|trainer.py:3801] 2025-04-13 22:28:26,116 >> Saving model checkpoint to /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-13 22:28:26,122 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/config.json
2025-04-13 22:28:21 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1
2025-04-13 22:28:21 - INFO - __main__ - Pushing to hub...
[INFO|configuration_utils.py:865] 2025-04-13 22:28:26,124 >> Configuration saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-13 22:29:41,128 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-13 22:29:41,133 >> tokenizer config file saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-13 22:29:41,135 >> Special tokens file saved in /beacon-scratch/tzheng24//Llama-3.1-8B-It/FL_1000_MoT_mixed_direct/OP_final_v2_10_2_3Rounds/ft_iter_1/special_tokens_map.json
2025-04-13 22:30:23 - INFO - __main__ - *** Training complete ***
