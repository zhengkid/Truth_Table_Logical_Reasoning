100%|██████████| 12/12 [12:23<00:00, 60.98s/it][INFO|trainer.py:2657] 2025-03-06 15:14:42,690 >>
{'loss': 0.2085, 'grad_norm': 1.7388669169734488, 'learning_rate': 4.914814565722671e-06, 'epoch': 0.15}
{'loss': 0.1932, 'grad_norm': 1.1387512414641, 'learning_rate': 3.147047612756302e-06, 'epoch': 0.73}
{'loss': 0.2177, 'grad_norm': 0.4312331296968297, 'learning_rate': 3.3493649053890325e-07, 'epoch': 1.59}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 12/12 [12:23<00:00, 61.97s/it]
{'train_runtime': 744.6899, 'train_samples_per_second': 2.339, 'train_steps_per_second': 0.016, 'train_loss': 0.20207941035429636, 'epoch': 1.88}
***** train metrics *****
  epoch                    =     1.8807
  total_flos               = 53667578GF
  train_loss               =     0.2021
  train_runtime            = 0:12:24.68
  train_samples            =        871
  train_samples_per_second =      2.339
  train_steps_per_second   =      0.016
2025-03-06 15:14:42 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3942] 2025-03-06 15:14:47,278 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:423] 2025-03-06 15:14:49,527 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:909] 2025-03-06 15:14:49,531 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3048] 2025-03-06 15:16:17,905 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-03-06 15:16:17,911 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-03-06 15:16:17,915 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/special_tokens_map.json
[INFO|trainer.py:3942] 2025-03-06 15:16:22,726 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:423] 2025-03-06 15:16:27,120 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:909] 2025-03-06 15:16:27,610 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3048] 2025-03-06 15:17:58,049 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-03-06 15:17:58,054 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-03-06 15:17:58,057 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/special_tokens_map.json
events.out.tfevents.1741291338.cbcb27.umiacs.umd.edu.72546.0: 100%|██████████| 6.96k/6.96k [00:06<00:00, 1.06kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 23.6MB/s]6<538:32:16, 2.51kB/s]
training_args.bin: 100%|██████████| 7.22k/7.22k [00:00<00:00, 84.6kB/s]5:57, 5.08MB/s]    
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:30<00:00, 36.1MB/s]    
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [02:02<00:00, 35.2MB/s]    
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [02:26<00:00, 33.6MB/s]     [00:06<00:00, 1.07kB/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [02:49<00:00, 28.8MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [02:49<00:00, 24.25s/it] :49<00:00, 30.1MB/s]
2025-03-06 15:21:24 - INFO - __main__ - Model saved to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:423] 2025-03-06 15:21:24,108 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/config.json
2025-03-06 15:21:24 - INFO - __main__ - Pushing to hub...
[INFO|trainer.py:3942] 2025-03-06 15:21:28,552 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:423] 2025-03-06 15:21:28,568 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:909] 2025-03-06 15:21:28,575 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3048] 2025-03-06 15:22:57,415 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-03-06 15:22:57,420 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-03-06 15:22:57,423 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct-star-code-v3_10-2-3Rounds-iter-3/code/v3_10_2_3Rounds/ft_iter_1/special_tokens_map.json
2025-03-06 15:23:37 - INFO - __main__ - *** Training complete ***
