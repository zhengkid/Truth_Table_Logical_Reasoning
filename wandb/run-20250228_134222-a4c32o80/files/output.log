100%|██████████| 5/5 [03:01<00:00, 35.04s/it][INFO|trainer.py:2657] 2025-02-28 13:45:24,882 >>
{'loss': 1.3095, 'learning_rate': 4.522542485937369e-06, 'epoch': 1.0}
{'loss': 1.1166, 'grad_norm': 22.8277053138044, 'learning_rate': 0.0, 'epoch': 5.0}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 5/5 [03:01<00:00, 36.32s/it]
{'train_runtime': 182.6188, 'train_samples_per_second': 1.588, 'train_steps_per_second': 0.027, 'train_loss': 1.1551473617553711, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               =  9258865GF
  train_loss               =     1.1551
  train_runtime            = 0:03:02.61
  train_samples            =         58
  train_samples_per_second =      1.588
  train_steps_per_second   =      0.027
2025-02-28 13:45:24 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3942] 2025-02-28 13:45:33,372 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-02-28 13:45:33,540 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-02-28 13:45:33,587 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-02-28 13:47:06,889 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-02-28 13:47:06,944 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-28 13:47:06,988 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/special_tokens_map.json
[INFO|trainer.py:3942] 2025-02-28 13:47:15,686 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-02-28 13:47:15,822 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-02-28 13:47:15,873 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-02-28 13:48:49,226 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-02-28 13:48:49,230 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-28 13:48:49,243 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/special_tokens_map.json
events.out.tfevents.1740768142.cbcb27.umiacs.umd.edu.626824.0: 100%|██████████| 6.49k/6.49k [00:00<00:00, 79.0kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 38.8MB/s]0<01:48, 44.9MB/s]
training_args.bin: 100%|██████████| 7.10k/7.10k [00:00<00:00, 103kB/s]02:18, 34.9MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:29<00:00, 36.8MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:51<00:00, 38.9MB/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [02:01<00:00, 40.2MB/s].49k [00:00<?, ?B/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [02:10<00:00, 37.9MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [02:10<00:00, 18.63s/it]2:10<00:00, 60.8MB/s]
[INFO|configuration_utils.py:423] 2025-02-28 13:52:27,121 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/config.json
[INFO|trainer.py:3942] 2025-02-28 13:52:39,754 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-02-28 13:52:39,769 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-02-28 13:52:39,781 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/generation_config.json
2025-02-28 13:52:26 - INFO - __main__ - Model saved to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2
2025-02-28 13:52:27 - INFO - __main__ - Pushing to hub...
[INFO|modeling_utils.py:3048] 2025-02-28 13:54:04,884 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-02-28 13:54:04,889 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-28 13:54:04,892 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/3Rounds/ft_iter_2/special_tokens_map.json
2025-02-28 13:54:33 - INFO - __main__ - *** Training complete ***
