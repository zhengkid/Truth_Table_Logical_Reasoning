100%|██████████| 34/34 [12:18<00:00, 21.72s/it][INFO|trainer.py:2584] 2025-04-14 01:13:01,958 >>
{'loss': 1.4843, 'grad_norm': 32.02452677250445, 'learning_rate': 4.989335440737587e-06, 'epoch': 0.06}
{'loss': 0.8195, 'grad_norm': 11.657405522480571, 'learning_rate': 4.737908228387656e-06, 'epoch': 0.29}
{'loss': 0.3469, 'grad_norm': 2.8420076764348776, 'learning_rate': 4.006586590948141e-06, 'epoch': 0.58}
{'loss': 0.2608, 'grad_norm': 0.576465915445078, 'learning_rate': 2.9593737945414264e-06, 'epoch': 0.87}
{'loss': 0.2754, 'grad_norm': 0.5574320908230456, 'learning_rate': 1.8158425248197931e-06, 'epoch': 1.16}
{'loss': 0.2254, 'grad_norm': 0.5703186903631079, 'learning_rate': 8.157608908836071e-07, 'epoch': 1.45}
{'loss': 0.2191, 'grad_norm': 0.5286467137790013, 'learning_rate': 1.6881942648911077e-07, 'epoch': 1.74}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 34/34 [12:18<00:00, 21.73s/it]
{'train_runtime': 739.7368, 'train_samples_per_second': 5.97, 'train_steps_per_second': 0.046, 'train_loss': 0.3607891394811518, 'epoch': 1.97}
***** train metrics *****
  epoch                    =      1.971
  total_flos               =     8299GF
  train_loss               =     0.3608
  train_runtime            = 0:12:19.73
  train_samples            =       2208
  train_samples_per_second =       5.97
  train_steps_per_second   =      0.046
2025-04-14 01:13:01 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-14 01:13:06,313 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-14 01:13:06,321 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-14 01:13:06,324 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 01:14:15,752 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 01:14:15,756 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 01:14:15,758 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-14 01:14:20,304 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-14 01:14:20,311 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-14 01:14:20,314 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 01:15:30,898 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 01:15:30,901 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 01:15:30,903 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/special_tokens_map.json
events.out.tfevents.1744606842.h1compute00.ihc.umd.edu.1392388.0: 100%|██████████| 7.69k/7.69k [00:00<00:00, 47.8kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 46.9MB/s]0<01:29, 54.0MB/s]
training_args.bin: 100%|██████████| 7.16k/7.16k [00:00<00:00, 238kB/s]01:17, 62.9MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:22<00:00, 49.0MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:21<00:00, 53.4MB/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [01:32<00:00, 52.7MB/s]0/7.69k [00:00<?, ?B/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:34<00:00, 52.3MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:34<00:00, 13.49s/it]1:34<00:00, 63.2MB/s]
[INFO|configuration_utils.py:414] 2025-04-14 01:17:45,813 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/config.json
[INFO|trainer.py:3801] 2025-04-14 01:17:49,873 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-14 01:17:49,879 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-14 01:17:49,881 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/generation_config.json
2025-04-14 01:17:45 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1
2025-04-14 01:17:45 - INFO - __main__ - Pushing to hub...
[INFO|modeling_utils.py:3042] 2025-04-14 01:19:04,211 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 01:19:04,214 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 01:19:04,216 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_1/special_tokens_map.json
2025-04-14 01:19:43 - INFO - __main__ - *** Training complete ***
