  0%|                                                                                                                                                                    | 0/6 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-02-23 19:30:45,805 >> It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
 33%|████████████████████████████████████████████████████                                                                                                        | 2/6 [00:30<00:59, 14.95s/it]
{'loss': 1.2123, 'grad_norm': 18.630912687685587, 'learning_rate': 2e-05, 'epoch': 0.36}
