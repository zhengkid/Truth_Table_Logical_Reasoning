100%|██████████| 95/95 [32:31<00:00, 20.52s/it][INFO|trainer.py:2584] 2025-04-15 04:34:06,656 >>
{'loss': 1.1852, 'grad_norm': 20.191519391668173, 'learning_rate': 4.998633143352315e-06, 'epoch': 0.05}
{'loss': 0.7307, 'grad_norm': 4.3426634011821905, 'learning_rate': 4.965903258506806e-06, 'epoch': 0.26}
{'loss': 0.2939, 'grad_norm': 1.6878019010311764, 'learning_rate': 4.864543104251587e-06, 'epoch': 0.51}
{'loss': 0.2232, 'grad_norm': 0.8893525375636784, 'learning_rate': 4.698684378016223e-06, 'epoch': 0.77}
{'loss': 0.2242, 'grad_norm': 2476.216088834575, 'learning_rate': 4.472851273490985e-06, 'epoch': 1.04}
{'loss': 0.1829, 'grad_norm': 0.6312216691299977, 'learning_rate': 4.1932039290643534e-06, 'epoch': 1.29}
{'loss': 0.1804, 'grad_norm': 0.5779732616076305, 'learning_rate': 3.8673703953060685e-06, 'epoch': 1.55}
{'loss': 0.1792, 'grad_norm': 0.6375903863544627, 'learning_rate': 3.5042385616324243e-06, 'epoch': 1.81}
{'loss': 0.2021, 'grad_norm': 0.5174954702349424, 'learning_rate': 3.1137137178519983e-06, 'epoch': 2.08}
{'loss': 0.1628, 'grad_norm': 0.5292695397024187, 'learning_rate': 2.7064483636808314e-06, 'epoch': 2.33}
{'loss': 0.1623, 'grad_norm': 0.47143029324943186, 'learning_rate': 2.2935516363191695e-06, 'epoch': 2.59}
{'loss': 0.167, 'grad_norm': 0.5301630850093937, 'learning_rate': 1.8862862821480023e-06, 'epoch': 2.84}
{'loss': 0.182, 'grad_norm': 0.4120770607129633, 'learning_rate': 1.495761438367577e-06, 'epoch': 3.12}
{'loss': 0.1528, 'grad_norm': 0.4770977387616738, 'learning_rate': 1.1326296046939334e-06, 'epoch': 3.37}
{'loss': 0.1515, 'grad_norm': 0.5679638993164524, 'learning_rate': 8.067960709356479e-07, 'epoch': 3.63}
{'loss': 0.1532, 'grad_norm': 0.39393213374036856, 'learning_rate': 5.271487265090163e-07, 'epoch': 3.88}
{'loss': 0.1715, 'grad_norm': 0.4216635997009573, 'learning_rate': 3.0131562198377763e-07, 'epoch': 4.15}
{'loss': 0.1557, 'grad_norm': 0.41107074900389434, 'learning_rate': 1.3545689574841341e-07, 'epoch': 4.41}
{'loss': 0.1437, 'grad_norm': 0.3782563174236879, 'learning_rate': 3.4096741493194196e-08, 'epoch': 4.66}
{'loss': 0.1454, 'grad_norm': 0.38844232981434007, 'learning_rate': 0.0, 'epoch': 4.92}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 95/95 [32:31<00:00, 20.54s/it]
{'train_runtime': 1954.1453, 'train_samples_per_second': 6.397, 'train_steps_per_second': 0.049, 'train_loss': 0.21344757519270244, 'epoch': 4.92}
***** train metrics *****
  epoch                    =     4.9201
  total_flos               =    32542GF
  train_loss               =     0.2134
  train_runtime            = 0:32:34.14
  train_samples            =       2500
  train_samples_per_second =      6.397
  train_steps_per_second   =      0.049
2025-04-15 04:34:06 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-15 04:34:10,844 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-15 04:34:10,852 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-15 04:34:10,854 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-15 04:35:20,530 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-15 04:35:20,534 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-15 04:35:20,536 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-15 04:35:24,902 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-15 04:35:24,908 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-15 04:35:24,911 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-15 04:36:35,747 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-15 04:36:35,790 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-15 04:36:35,794 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/special_tokens_map.json
events.out.tfevents.1744704092.h1compute00.ihc.umd.edu.1518921.0: 100%|██████████| 10.5k/10.5k [00:00<00:00, 82.1kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 39.6MB/s]0<03:35, 22.6MB/s]
training_args.bin: 100%|██████████| 7.35k/7.35k [00:00<00:00, 233kB/s]03:14, 25.0MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:21<00:00, 50.6MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:33<00:00, 46.1MB/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.88G/4.88G [01:35<00:00, 51.2MB/s]0/10.5k [00:00<?, ?B/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:42<00:00, 48.1MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:42<00:00, 14.70s/it]1:42<00:00, 59.6MB/s]
[INFO|configuration_utils.py:414] 2025-04-15 04:38:58,480 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/config.json
[INFO|trainer.py:3801] 2025-04-15 04:39:02,292 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-15 04:39:02,297 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-15 04:39:02,300 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/generation_config.json
2025-04-15 04:38:58 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1
2025-04-15 04:38:58 - INFO - __main__ - Pushing to hub...
[INFO|modeling_utils.py:3042] 2025-04-15 04:40:13,568 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-15 04:40:13,571 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-15 04:40:13,573 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/PW_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_1/special_tokens_map.json
2025-04-15 04:40:51 - INFO - __main__ - *** Training complete ***
