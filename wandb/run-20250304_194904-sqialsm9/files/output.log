100%|██████████| 35/35 [31:35<00:00, 54.51s/it][INFO|trainer.py:2657] 2025-03-04 20:20:40,520 >>
{'loss': 0.9799, 'grad_norm': 13.955065906085872, 'learning_rate': 4.989935734988098e-06, 'epoch': 0.14}
{'loss': 0.6089, 'grad_norm': 7.784218457934435, 'learning_rate': 4.752422169756048e-06, 'epoch': 0.7}
{'loss': 0.2674, 'grad_norm': 4.375202225442351, 'learning_rate': 4.058724504646834e-06, 'epoch': 1.28}
{'loss': 0.2096, 'grad_norm': 0.5367541289446838, 'learning_rate': 3.056302334890786e-06, 'epoch': 1.98}
{'loss': 0.1927, 'grad_norm': 0.472111787717903, 'learning_rate': 1.9436976651092143e-06, 'epoch': 2.56}
{'loss': 0.1793, 'grad_norm': 1.1354333712538285, 'learning_rate': 9.412754953531664e-07, 'epoch': 3.14}
{'loss': 0.1717, 'grad_norm': 0.37931639817909907, 'learning_rate': 2.4757783024395244e-07, 'epoch': 3.84}
{'loss': 0.1699, 'grad_norm': 0.3944229879903239, 'learning_rate': 0.0, 'epoch': 4.42}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 35/35 [31:35<00:00, 54.16s/it]
{'train_runtime': 1896.4902, 'train_samples_per_second': 2.399, 'train_steps_per_second': 0.018, 'train_loss': 0.2676696811403547, 'epoch': 4.42}
***** train metrics *****
  epoch                    =      4.4211
  total_flos               = 139606613GF
  train_loss               =      0.2677
  train_runtime            =  0:31:36.49
  train_samples            =         910
  train_samples_per_second =       2.399
  train_steps_per_second   =       0.018
2025-03-04 20:20:40 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3942] 2025-03-04 20:21:12,034 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-03-04 20:21:12,049 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-03-04 20:21:12,055 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-03-04 20:22:53,394 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-03-04 20:22:53,400 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-03-04 20:22:53,403 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/special_tokens_map.json
[INFO|trainer.py:3942] 2025-03-04 20:23:04,930 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-03-04 20:23:04,943 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-03-04 20:23:04,946 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-03-04 20:25:19,256 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-03-04 20:25:19,263 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-03-04 20:25:19,268 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/special_tokens_map.json
events.out.tfevents.1741135744.cbcb27.umiacs.umd.edu.1508487.0: 100%|██████████| 7.81k/7.81k [00:00<00:00, 84.5kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 37.5MB/s]<?, ?B/s]
training_args.bin: 100%|██████████| 7.16k/7.16k [00:00<00:00, 41.6kB/s]1:04, 74.5MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:36<00:00, 29.7MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [02:28<00:00, 29.2MB/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [02:38<00:00, 30.8MB/s]7.81k [00:00<?, ?B/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [02:57<00:00, 27.8MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [02:57<00:00, 25.38s/it] :37<00:15, 36.6MB/s]
[INFO|configuration_utils.py:423] 2025-03-04 20:29:18,819 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/config.json
2025-03-04 20:29:13 - INFO - __main__ - Model saved to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2
2025-03-04 20:29:18 - INFO - __main__ - Pushing to hub...
[INFO|trainer.py:3942] 2025-03-04 20:29:23,266 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-03-04 20:29:23,280 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-03-04 20:29:23,289 >> Configuration saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-03-04 20:30:59,200 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-03-04 20:30:59,205 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-03-04 20:30:59,208 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Qwen2.5-7B-Instruct/code/v3_10_3Rounds/ft_iter_2/special_tokens_map.json
2025-03-04 20:32:04 - INFO - __main__ - *** Training complete ***
