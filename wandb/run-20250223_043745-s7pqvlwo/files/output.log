  0%|                                                                                                                                                                    | 0/3 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-02-23 04:37:46,087 >> It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  3.76s/it][INFO|trainer.py:2584] 2025-02-23 04:38:00,083 >>
{'loss': 0.9856, 'learning_rate': 2e-05, 'epoch': 1.0}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.67s/it]
{'train_runtime': 14.9286, 'train_samples_per_second': 0.201, 'train_steps_per_second': 0.201, 'train_loss': 0.9856093525886536, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  total_flos               =      221GF
  train_loss               =     0.9856
  train_runtime            = 0:00:14.92
  train_samples            =          6
  train_samples_per_second =      0.201
  train_steps_per_second   =      0.201
2025-02-23 04:38:00 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-02-23 04:38:04,423 >> Saving model checkpoint to data/zephyr-7b-gemma-sft
[INFO|configuration_utils.py:414] 2025-02-23 04:38:04,429 >> Configuration saved in data/zephyr-7b-gemma-sft/config.json
[INFO|configuration_utils.py:865] 2025-02-23 04:38:04,432 >> Configuration saved in data/zephyr-7b-gemma-sft/generation_config.json
