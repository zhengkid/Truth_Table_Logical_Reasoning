  0%|                                                                                                                                                                   | 0/50 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-02-24 23:27:53,706 >> It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
  4%|██████▏                                                                                                                                                    | 2/50 [00:54<21:34, 26.97s/it]
{'loss': 1.4646, 'grad_norm': 32.83831847768866, 'learning_rate': 4.9950668210706795e-06, 'epoch': 0.09}
