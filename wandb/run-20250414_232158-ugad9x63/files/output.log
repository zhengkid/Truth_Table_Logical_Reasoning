100%|██████████| 105/105 [36:14<00:00, 20.66s/it][INFO|trainer.py:2584] 2025-04-14 23:58:14,024 >>
{'loss': 0.0224, 'grad_norm': 0.4854949307758435, 'learning_rate': 4.998881080708759e-06, 'epoch': 0.05}
{'loss': 0.0246, 'grad_norm': 0.8132274185324306, 'learning_rate': 4.9720770655628216e-06, 'epoch': 0.23}
{'loss': 0.03, 'grad_norm': 0.7820057765725432, 'learning_rate': 4.8889320144653525e-06, 'epoch': 0.46}
{'loss': 0.0306, 'grad_norm': 0.644480320388049, 'learning_rate': 4.752422169756048e-06, 'epoch': 0.69}
{'loss': 0.0326, 'grad_norm': 0.5517971365802044, 'learning_rate': 4.565596935789987e-06, 'epoch': 0.92}
{'loss': 0.0286, 'grad_norm': 0.5427755426095915, 'learning_rate': 4.332629679574566e-06, 'epoch': 1.16}
{'loss': 0.0218, 'grad_norm': 0.5277274040052168, 'learning_rate': 4.058724504646834e-06, 'epoch': 1.39}
{'loss': 0.0222, 'grad_norm': 0.5787948989294098, 'learning_rate': 3.7500000000000005e-06, 'epoch': 1.62}
{'loss': 0.0223, 'grad_norm': 0.44852546679055133, 'learning_rate': 3.4133525609159883e-06, 'epoch': 1.85}
{'loss': 0.0208, 'grad_norm': 0.3076706842835546, 'learning_rate': 3.056302334890786e-06, 'epoch': 2.09}
{'loss': 0.0146, 'grad_norm': 0.3740797947006216, 'learning_rate': 2.686825233966061e-06, 'epoch': 2.32}
{'loss': 0.0151, 'grad_norm': 0.3665936201426339, 'learning_rate': 2.3131747660339396e-06, 'epoch': 2.55}
{'loss': 0.0145, 'grad_norm': 0.3179410676858444, 'learning_rate': 1.9436976651092143e-06, 'epoch': 2.78}
{'loss': 0.0155, 'grad_norm': 0.44707705652797575, 'learning_rate': 1.5866474390840126e-06, 'epoch': 3.02}
{'loss': 0.0114, 'grad_norm': 0.20073870933224588, 'learning_rate': 1.2500000000000007e-06, 'epoch': 3.25}
{'loss': 0.0115, 'grad_norm': 0.19560951281880398, 'learning_rate': 9.412754953531664e-07, 'epoch': 3.48}
{'loss': 0.0116, 'grad_norm': 0.24014191744753452, 'learning_rate': 6.673703204254348e-07, 'epoch': 3.71}
{'loss': 0.0116, 'grad_norm': 0.18586426954473753, 'learning_rate': 4.344030642100133e-07, 'epoch': 3.95}
{'loss': 0.0119, 'grad_norm': 0.1327706974897438, 'learning_rate': 2.4757783024395244e-07, 'epoch': 4.18}
{'loss': 0.0104, 'grad_norm': 0.1406219430768987, 'learning_rate': 1.1106798553464804e-07, 'epoch': 4.41}
{'loss': 0.0106, 'grad_norm': 0.16041736229045578, 'learning_rate': 2.7922934437178695e-08, 'epoch': 4.65}
{'loss': 0.0106, 'grad_norm': 0.13355121966658934, 'learning_rate': 0.0, 'epoch': 4.88}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 105/105 [36:14<00:00, 20.71s/it]
{'train_runtime': 2175.8375, 'train_samples_per_second': 6.379, 'train_steps_per_second': 0.048, 'train_loss': 0.018196561542295275, 'epoch': 4.88}
***** train metrics *****
  epoch                    =     4.8761
  total_flos               =    26850GF
  train_loss               =     0.0182
  train_runtime            = 0:36:15.83
  train_samples            =       2776
  train_samples_per_second =      6.379
  train_steps_per_second   =      0.048
2025-04-14 23:58:14 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-14 23:58:18,326 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 23:58:18,334 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 23:58:18,336 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 23:59:26,585 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 23:59:26,589 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 23:59:26,591 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-14 23:59:31,033 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 23:59:31,040 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 23:59:31,042 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-15 00:00:51,929 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-15 00:00:51,932 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-15 00:00:51,934 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/special_tokens_map.json
events.out.tfevents.1744687318.h1compute00.ihc.umd.edu.1499518.0: 100%|██████████| 11.0k/11.0k [00:00<00:00, 202kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 20.1MB/s]0<01:30, 53.3MB/s]
training_args.bin: 100%|██████████| 7.35k/7.35k [00:00<00:00, 78.0kB/s]1:45, 45.9MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:21<00:00, 50.7MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:38<00:00, 43.8MB/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:48<00:00, 45.4MB/s]0/11.0k [00:00<?, ?B/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.88G/4.88G [01:50<00:00, 44.1MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:50<00:00, 15.82s/it]
[INFO|configuration_utils.py:414] 2025-04-15 00:03:30,671 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/config.json
[INFO|trainer.py:3801] 2025-04-15 00:03:34,675 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-15 00:03:34,681 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-15 00:03:34,684 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/generation_config.json
2025-04-15 00:03:30 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3
2025-04-15 00:03:30 - INFO - __main__ - Pushing to hub...
[INFO|modeling_utils.py:3042] 2025-04-15 00:04:45,969 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-15 00:04:45,972 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-15 00:04:45,974 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B/FL_1000_MoT5_mixed_direct/OP_final_v2_10_5_3Rounds/ft_iter_3/special_tokens_map.json
2025-04-15 00:05:39 - INFO - __main__ - *** Training complete ***
