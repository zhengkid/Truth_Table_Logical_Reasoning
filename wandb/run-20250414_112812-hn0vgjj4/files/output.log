100%|██████████| 40/40 [14:32<00:00, 21.83s/it][INFO|trainer.py:2584] 2025-04-14 11:42:45,659 >>
{'loss': 0.0694, 'grad_norm': 0.4243211451651487, 'learning_rate': 4.992293334332821e-06, 'epoch': 0.05}
{'loss': 0.0654, 'grad_norm': 0.7385472228884633, 'learning_rate': 4.809698831278217e-06, 'epoch': 0.25}
{'loss': 0.0608, 'grad_norm': 0.5900989620044731, 'learning_rate': 4.267766952966369e-06, 'epoch': 0.49}
{'loss': 0.0587, 'grad_norm': 0.5155754604278525, 'learning_rate': 3.4567085809127247e-06, 'epoch': 0.74}
{'loss': 0.0565, 'grad_norm': 0.4918952415542097, 'learning_rate': 2.5e-06, 'epoch': 0.98}
{'loss': 0.047, 'grad_norm': 0.3752764215591826, 'learning_rate': 1.5432914190872757e-06, 'epoch': 1.23}
{'loss': 0.0369, 'grad_norm': 0.36289503393714895, 'learning_rate': 7.322330470336314e-07, 'epoch': 1.47}
{'loss': 0.0368, 'grad_norm': 0.37355948440321624, 'learning_rate': 1.9030116872178317e-07, 'epoch': 1.72}
{'loss': 0.0346, 'grad_norm': 0.35781267464785477, 'learning_rate': 0.0, 'epoch': 1.96}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 40/40 [14:32<00:00, 21.82s/it]
{'train_runtime': 873.9307, 'train_samples_per_second': 5.966, 'train_steps_per_second': 0.046, 'train_loss': 0.04968522824347019, 'epoch': 1.96}
***** train metrics *****
  epoch                    =     1.9632
  total_flos               =     9913GF
  train_loss               =     0.0497
  train_runtime            = 0:14:33.93
  train_samples            =       2607
  train_samples_per_second =      5.966
  train_steps_per_second   =      0.046
2025-04-14 11:42:45 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-14 11:42:49,846 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 11:42:49,854 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 11:42:49,857 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 11:43:58,579 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 11:43:58,583 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 11:43:58,584 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-14 11:44:04,078 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 11:44:04,085 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 11:44:04,087 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 11:45:13,718 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 11:45:13,722 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 11:45:13,724 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/special_tokens_map.json
events.out.tfevents.1744644491.h1compute00.ihc.umd.edu.1433258.0: 100%|██████████| 8.19k/8.19k [00:00<00:00, 54.4kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 53.1MB/s]<19:47, 4.11MB/s]
training_args.bin: 100%|██████████| 7.16k/7.16k [00:00<00:00, 231kB/s]01:07, 71.3MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:20<00:00, 52.3MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:14<00:00, 57.9MB/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:24<00:00, 58.4MB/s]0/8.19k [00:00<?, ?B/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [01:26<00:00, 56.1MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:27<00:00, 12.46s/it]1:26<00:00, 68.1MB/s]
2025-04-14 11:47:28 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 11:47:28,060 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/config.json
2025-04-14 11:47:28 - INFO - __main__ - Pushing to hub...
[INFO|trainer.py:3801] 2025-04-14 11:47:32,019 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3
[INFO|configuration_utils.py:414] 2025-04-14 11:47:32,025 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/config.json
[INFO|configuration_utils.py:865] 2025-04-14 11:47:32,029 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 11:48:42,611 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 11:48:42,615 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 11:48:42,617 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_5_2_3Rounds/ft_iter_3/special_tokens_map.json
2025-04-14 11:49:21 - INFO - __main__ - *** Training complete ***
