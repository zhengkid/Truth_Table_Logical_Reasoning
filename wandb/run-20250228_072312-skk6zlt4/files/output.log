100%|██████████| 10/10 [10:36<00:00, 62.08s/it][INFO|trainer.py:2657] 2025-02-28 07:33:50,055 >>
{'loss': 1.0434, 'grad_norm': 17.502242251060636, 'learning_rate': 4.8776412907378845e-06, 'epoch': 0.44}
{'loss': 0.6355, 'grad_norm': 6.017610000560162, 'learning_rate': 2.5e-06, 'epoch': 1.89}
{'loss': 0.2494, 'grad_norm': 7.287133077521603, 'learning_rate': 0.0, 'epoch': 3.44}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 10/10 [10:36<00:00, 63.67s/it]
{'train_runtime': 637.6316, 'train_samples_per_second': 2.227, 'train_steps_per_second': 0.016, 'train_loss': 0.4832197308540344, 'epoch': 3.44}
***** train metrics *****
  epoch                    =     3.4444
  total_flos               = 42016474GF
  train_loss               =     0.4832
  train_runtime            = 0:10:37.63
  train_samples            =        284
  train_samples_per_second =      2.227
  train_steps_per_second   =      0.016
2025-02-28 07:33:50 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3942] 2025-02-28 07:33:57,517 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-02-28 07:33:57,575 >> Configuration saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-02-28 07:33:57,580 >> Configuration saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-02-28 07:36:35,687 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-02-28 07:36:35,695 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-28 07:36:35,698 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/special_tokens_map.json
[INFO|trainer.py:3942] 2025-02-28 07:36:42,678 >> Saving model checkpoint to /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2
[INFO|configuration_utils.py:423] 2025-02-28 07:36:42,719 >> Configuration saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/config.json
[INFO|configuration_utils.py:909] 2025-02-28 07:36:42,724 >> Configuration saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/generation_config.json
[INFO|modeling_utils.py:3048] 2025-02-28 07:38:57,100 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2500] 2025-02-28 07:38:57,107 >> tokenizer config file saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-28 07:38:57,110 >> Special tokens file saved in /fs/cbcb-lab/heng/kz//Meta-Llama-3.1-8B-Instruct/code/3Rounds/ft_iter_2/special_tokens_map.json
