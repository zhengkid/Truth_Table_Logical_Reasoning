100%|██████████| 32/32 [11:45<00:00, 22.06s/it][INFO|trainer.py:2584] 2025-04-14 13:02:16,973 >>
{'loss': 1.46, 'grad_norm': 31.228884383337643, 'learning_rate': 4.987961816680493e-06, 'epoch': 0.06}
{'loss': 0.8158, 'grad_norm': 12.556360603683633, 'learning_rate': 4.704803160870888e-06, 'epoch': 0.31}
{'loss': 0.3521, 'grad_norm': 2.0567684674496163, 'learning_rate': 3.888925582549006e-06, 'epoch': 0.61}
{'loss': 0.2645, 'grad_norm': 0.621149313606719, 'learning_rate': 2.7450428508239024e-06, 'epoch': 0.92}
{'loss': 0.2696, 'grad_norm': 0.6329329149695562, 'learning_rate': 1.5432914190872757e-06, 'epoch': 1.23}
{'loss': 0.2296, 'grad_norm': 0.529332707128334, 'learning_rate': 5.674738665931575e-07, 'epoch': 1.53}
{'loss': 0.2232, 'grad_norm': 0.515343396677377, 'learning_rate': 4.8036798991923925e-08, 'epoch': 1.84}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 32/32 [11:45<00:00, 22.04s/it]
{'train_runtime': 706.6016, 'train_samples_per_second': 5.907, 'train_steps_per_second': 0.045, 'train_loss': 0.37101117242127657, 'epoch': 1.96}
***** train metrics *****
  epoch                    =     1.9617
  total_flos               =     7817GF
  train_loss               =      0.371
  train_runtime            = 0:11:46.60
  train_samples            =       2087
  train_samples_per_second =      5.907
  train_steps_per_second   =      0.045
2025-04-14 13:02:16 - INFO - __main__ - *** Save model ***
[INFO|trainer.py:3801] 2025-04-14 13:02:21,199 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-14 13:02:21,206 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-14 13:02:21,208 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 13:03:28,659 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 13:03:28,664 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 13:03:28,666 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/special_tokens_map.json
[INFO|trainer.py:3801] 2025-04-14 13:03:33,078 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-14 13:03:33,085 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-14 13:03:33,087 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/generation_config.json
[INFO|modeling_utils.py:3042] 2025-04-14 13:04:40,828 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 13:04:40,834 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 13:04:40,836 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/special_tokens_map.json
events.out.tfevents.1744649430.h1compute00.ihc.umd.edu.1440028.0: 100%|██████████| 7.69k/7.69k [00:00<00:00, 131kB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 39.5MB/s]0<01:42, 47.4MB/s]
training_args.bin: 100%|██████████| 7.16k/7.16k [00:00<00:00, 162kB/s]01:42, 47.1MB/s]
model-00004-of-00004.safetensors: 100%|██████████| 1.09G/1.09G [00:18<00:00, 60.4MB/s]
model-00003-of-00004.safetensors: 100%|██████████| 4.33G/4.33G [01:25<00:00, 50.5MB/s]
model-00002-of-00004.safetensors: 100%|██████████| 4.93G/4.93G [01:28<00:00, 55.7MB/s]0/7.69k [00:00<?, ?B/s]
model-00001-of-00004.safetensors: 100%|██████████| 4.87G/4.87G [01:30<00:00, 54.0MB/s]
Upload 7 LFS files: 100%|██████████| 7/7 [01:30<00:00, 12.92s/it]
[INFO|configuration_utils.py:414] 2025-04-14 13:06:51,451 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/config.json
[INFO|trainer.py:3801] 2025-04-14 13:06:55,274 >> Saving model checkpoint to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1
[INFO|configuration_utils.py:414] 2025-04-14 13:06:55,281 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/config.json
[INFO|configuration_utils.py:865] 2025-04-14 13:06:55,285 >> Configuration saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/generation_config.json
2025-04-14 13:06:51 - INFO - __main__ - Model saved to /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1
2025-04-14 13:06:51 - INFO - __main__ - Pushing to hub...
[INFO|modeling_utils.py:3042] 2025-04-14 13:08:05,868 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2646] 2025-04-14 13:08:05,871 >> tokenizer config file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-04-14 13:08:05,873 >> Special tokens file saved in /beacon-scratch/tzheng24//Qwen2.5-7B-Instruct/FL_1000_MoT_mixed_direct/OP_final_v2_3_2_3Rounds/ft_iter_1/special_tokens_map.json
2025-04-14 13:08:44 - INFO - __main__ - *** Training complete ***
